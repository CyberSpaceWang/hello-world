# 机器学习中的优化

==王宇飞==

该文档主要总结机器学习中常见的优化问题与求解算法。



# 梯度下降方法

经典的机器学习问题中，目标函数通常由如下方式表示：
$$
L(\theta)=\mathbb{E}_{(x, y) \sim P_{\text {data }}} L(f(x, \theta), y)
$$
该模型刻画了当参数为 $\theta$ 时，模型在所有数据上的平均损失。

- **经典的梯度下降算法**：采用训练数据的**平均**损失来近似：

$$
L(\theta)=\frac{1}{M} \sum_{i=1}^{M} L\left(f\left(x_{i}, \theta\right), y_{i}\right)
$$

$$
\nabla L(\theta)=\frac{1}{M} \sum_{i=1}^{M} \nabla L\left(f\left(x_{i}, \theta\right), y_{i}\right)
$$

其中M表示数据量，$f$为模型输出，$y$ 为目标输出，参数更新公式为：
$$
\theta_{t+1}=\theta_{t}-\alpha \nabla L\left(\theta_{t}\right)
$$
　　可以看出，经典的梯度下降算法对模型参数进行更新时，需要遍历所有的训练数据，M很大时，这需要很大的计算量。**耗费很长的时间**，在实际应用中不可行。为了解决这个问题，引入随机梯度下降方法。

- **随机梯度下降算法**：用**单个样本**的损失来近似平均损失：

$$
L\left(\theta ; x_{i}, y_{i}\right)=L\left(f\left(x_{i}, \theta\right), y_{i}\right)
$$

使用随机梯度下降方法大大加快了收敛速率，该方法也适合数据的在线更新场景。同时，为了降低随机梯度的方差，也为了充分利用高度优化的矩阵运算，在实际应用中会同时处理若干训练数据，该方法被称为小批量随机梯度下降。

- **小批量随机梯度下降**：同时处理m个数据损失的方法：

$$
L(\theta)=\frac{1}{m} \sum_{j=1}^{m} L\left(f\left(x_{i_{j}}, \theta\right), y_{i_{j}}\right)
$$

>1. 如何选取参数m？
>
>    一般m取2的幂时能充分利用矩阵运算操作。
>
>2. 如何选取学习率$\alpha$ ？
>
>    一开始采用较大的学习率，当误差进入平台期时，减小学习率。

## 应用：线性回归

<img src="优化.assets/lr.svg" width=400>

线性回归是寻找最合适的参数$w$和$b$从而使得线性拟合数据效果最好，设定直线表达式如下：
$$
\hat{y}=wx+b
$$
损失函数$L$为均方误差函数，定义如下：
$$
L=\sum_{i=1}^{n}(\hat{y}-y)^2
$$

$$
\frac{\partial L}{\partial w}=2\sum_{i=1}^{n}(\hat{y}-y)\times x=2\sum_{i=1}^{n}(wx+b-y)\times x
$$

采用梯度下降进行更新：
$$
w=w-r\times\frac{\partial L}{\partial w}
$$

>1. **注意损失函数和目标函数不一样：**
>
>    对于线性回归问题，目标函数是固定$w$，通过不同的输入$x$ 计算$\hat{y}$，而损失函数则是当$x$ 固定后，计算梯度从而修改$w$ 的值。
>
>2. 若$L=L_1+L_2$，则$\frac{\partial L}{\partial w}=\frac{\partial L_1}{\partial w}+\frac{\partial L_2}{\partial w}$，$w=w-r\times (\frac{\partial L_1}{\partial w}+\frac{\partial L_2}{\partial w})$











